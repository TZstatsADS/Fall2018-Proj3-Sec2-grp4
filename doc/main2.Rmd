---
title: "Project 3 - Example Main Script"
author: "Ken, Han, Jay, Francy, and Zuleimy"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

In your final repo, there should be an R markdown file that organizes **all computational steps** for evaluating your proposed image classification framework. 

This file is currently a template for running evaluation experiments of image analysis (or any predictive modeling). You should update it according to your codes but following precisely the same structure. 

```{r,warning=FALSE}
if(!require("EBImage")){
  source("https://bioconductor.org/biocLite.R")
  biocLite("EBImage")
  library("EBImage")
}

if(!require("gbm")){
  install.packages("gbm")
  library("gbm")
}
if(!require("xgboost")){
  install.packages("xgboost")
}

library("EBImage")
library("gbm")
library("xgboost")
```

### Parallel Computing Setup
```{r ParallelSetup}
if(!require("doParallel")){
  install.packages("doParallel")
  library("doParallel")
}

# Real physical cores in the computer
cores <- detectCores()

if(cores>1){
  
  if(.Platform$OS.type=="windows"){
    cl <- makeCluster(cores)
    registerDoParallel(cl, cores=cores)
  }
  else{
    if(!require("doMC")){
      install.packages("doMC")
      library("doMC")
    }
    registerDoMC(cores)
  }
  
  run.parallel=TRUE

}else
  run.parallel=FALSE
```

### Step 0: specify directories.

Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. 

```{r wkdir}
set.seed(2018)
# setwd("./ads_fall2018_proj3") 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
```

Provide directories for training images. Low-resolution (LR) image set and High-resolution (HR) image set will be in different subfolders. 
```{r}
train_dir <- "../data/train_set/" # This will be modified for different data sets.
train_LR_dir <- paste(train_dir, "LR/", sep="")
train_HR_dir <- paste(train_dir, "HR/", sep="")
train_label_path <- paste(train_dir, "label.csv", sep="") 
```

### Step 1: set up controls for evaluation experiments.

In this chunk, we have a set of controls for the evaluation experiments. 

+ (T/F) cross-validation on the training set
+ (number) K, the number of CV folds
+ (T/F) process features for training set
+ (T/F) run evaluation on an independent test set
+ (T/F) process features for test set

```{r exp_setup}
run.cv=FALSE # run cross-validation on the training set
run.cv.xg = TRUE #run cross-validation for XGboost
K <- 5  # number of CV folds
run.train=FALSE # New switch, whether run training.
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.test.xg = TRUE # run evaluation on XGboost 
run.feature.test=TRUE # process features for test set
# run.parallel already set at ### Parallel Computing Setup
```

Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this example, we use GBM with different `depth`. In the following chunk, we list, in a vector, setups (in this case, `depth`) corresponding to models that we will compare. In your project, you might compare very different classifiers. You can assign them numerical IDs and labels specific to your project. 

```{r model_setup}
model_values <- seq(1, 11, 2)
model_labels = paste("GBM with depth =", model_values)
depth.list <- c(4,5,6,7,8,9,10,11)
eta.list <- c(0.02,0.04,0.06,0.08,0.1)
```

### Step 2: import training images class labels.

We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.

```{r train_label}
extra_label <- read.csv(train_label_path, colClasses=c("NULL", NA, NA))
```

### Step 3: construct features and responses

`feature.R` should be the wrapper for all your feature engineering functions and options. The function `feature( )` should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. 
+ `feature.R`
  + Input: a path for low-resolution images.
  + Input: a path for high-resolution images.
  + Output: an RData file that contains extracted features and corresponding responses

I do recommend using non-parallel version here, as extract feature is a data-intensive --- not computation intensive step. Thus, parallel computing won't significantly save time --- even increase running time, for the parallel computing will raise additional coordination cost, depends on different hardwares: 

| Running Time | HPC | Pixelbook | old laptop |
|--------------|:---:|:---------:|:----------:|
|   Parallel   | 19  | 55        | 80         |
| Non-Parallel | 51  | 33        | 90         |

```{r featuresource}
run.parallel=FALSE
if(run.parallel)
  source("../lib/feature_parallel.R") else
    source("../lib/feature.R")
```
```{r feature}
tm_feature_train <- NA
if(run.feature.train){
  tm0=proc.time()
  dat_train <- feature(train_LR_dir, train_HR_dir)
  tm_feature_train=proc.time()-tm0
  
  save(dat_train, file="../output/feature_train.RData")
}else{
  load("../output/feature_train.RData")
}

feat_train <- dat_train$feature
label_train <- dat_train$label
rm(dat_train)
tmp=gc() # release memory
```


### Step 4: Train a classification model with training images
Call the train model and test model from library. 

`train.R` and `test.R` should be wrappers for all your model training steps and your classification/prediction steps. 
+ `train.R`
  + Input: a path that points to the training set features and responses.
  + Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.
+ `test.R`
  + Input: a path that points to the test set features.
  + Input: an R object that contains a trained classifier.
  + Output: an R object of response predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. 
```{r loadlib}
if(run.parallel)
  source("../lib/train_parallel.R") else
    source("../lib/train.R")

### The test parallel will increase much more time, so we switch off (comment) it
# if(run.parallel)
#   source("../lib/test_parallel.R") else
    source("../lib/test.R")
```

#### Model selection with cross-validation
* Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. 
```{r runcv, message=FALSE, warning=FALSE}
source("../lib/cross_validation.R")
source("../lib/xg_cv.R")
if(run.cv){
  err_cv <- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat("k=", k, "\n")
    err_cv[k,] <- cv.function(feat_train, label_train, model_values[k], K)
    system("free -m")
  }
  save(err_cv, file="../output/err_cv.RData")
}else{
  load("../output/err_cv.RData")
}
tmp=gc() # release memory
####XGboost
if(run.cv.xg){
  
  par_matrix <- cv_real(feat_train,label_train)
  par <- par_matrix$parlist
  matrix <- par_matrix$matrix

}
```

Visualize cross-validation results. 
```{r cv_vis}
# if(run.cv){
  # load("../output/err_cv.RData")
  plot(model_values, err_cv[,1], xlab="Interaction Depth", ylab="CV Error",
       main="Cross Validation Error", type="n", ylim=c(0, 0.005))
  points(model_values, err_cv[,1], col="blue", pch=16)
  lines(model_values, err_cv[,1], col="blue")
  arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
# }
  
  
###############Xgboost Model
  if(run.cv.xg){
  par(mfrow=c(3,4))
  for(i in 1:12){
    parameter <- par[[i]]
    error <- parameter$error
    e <- 
    plot(depth.list,error[,1],type="o",col=1,xlab="depth",ylab="error rate",main="Cross Validation Error")
    for(j in 2:length(eta.list)){
      points(depth.list,error[,j],type="o",col=j)
    }
    legend("topright",fill=1:length(eta.list),legend=paste("eta=",eta.list),cex=0.25)
  }}
#plots below are the error for 12 classifiers 
```


* Choose the "best" parameter value
```{r best_model}
model_best=model_values[1]
# if(run.cv){
  model_best <- model_values[which.min(err_cv[,1])]
  cat("The min cv is",min(err_cv[,1]),"for depth",model_best,"\n")
# }

par_best <- list(depth=model_best)



if(run.cv.xg){
  par_best_list <- par 
}
```

* Train the model with the entire training set using the selected model (model parameter) via cross-validation.
```{r final_train}
source("../lib/train_xg.R")
if(run.train){
  tm0=proc.time()
  fit_train <- train(feat_train, label_train, par_best)
  tm_train=proc.time()-tm0
  save(fit_train, file="../output/fit_train.RData")
}else{
  load("../output/fit_train.RData")
}
tmp=gc() # release memory


###########Xgboost train
#url1 <- paste(getwd(),"/Fall2018-Proj3-Sec2-grp4/output/xgboost_models.RData",sep="")
#load(url1)
result <- train_x(matrix,par_best_list) 
time <- result$train_time
models_final <- result$models_final

#url1 <- paste(getwd(),"/Fall2018-Proj3-Sec2-grp4/output/xgboost_models.RData",sep="")
#save(models_final,file=url1)

```

### Step 5: Super-resolution for test images
Feed the final training model with the completely holdout testing data. 
+ `superResolution.R`
  + Input: a path that points to the folder of low-resolution test images.
  + Input: a path that points to the folder (empty) of high-resolution test images.
  + Input: an R object that contains tuned predictors.
  + Output: construct high-resolution versions for each low-resolution test image.
```{r superresolution}
run.parallel=FALSE
if(run.parallel)
  source("../lib/superResolution_parallel.R") else
    source("../lib/superResolution.R")
source("../lib/test.xg.R")
test_dir <- "../data/test_set/" # This will be modified for different data sets.
test_LR_dir <- paste(test_dir, "LR/", sep="")
test_HR_dir <- paste(test_dir, "HR/", sep="")

if(!run.test){
  tm0=proc.time()
  superResolution(test_LR_dir, test_HR_dir, fit_train)
  tm_test=proc.time()-tm0
}

###########XGboost
if(run.test.xg){
  url1 <- ("../output/xgboost_models.RData")
  load(url1)
  time_s <- test.xg(test_LR_dir,test_HR_dir,models_final)
}
```

### Summarize Running Time
Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. 
```{r running_time}
cat("Time for constructing training features=", tm_feature_train[3], "s \n")
#cat("Time for constructing testing features=", tm_feature_test[3], "s \n")
cat("Time for training model=", tm_train[3], "s \n")
cat("Time for super-resolution=", tm_test[3], "s \n")
cat("Time for training model(xgboost)=", sum(time), "s \n")
cat("Time for super-resolution(xgboost)=", sum(time_s), "s \n")
```

### Summarize Accuracy
```{r accuracy}
train_dir <- "../data/train_set/HR/"
test_dir <- "../data/test_set/HR/"
source("../lib/Evaluation.R")
mp=msepsnr(train_dir, test_dir)
cat("MSE is", mp[1])
cat(" PSNR is", mp[2])
```

### Stop Parallel Computing 
```{r stopParallel}
if(run.parallel & .Platform$OS.type=="windows"){
  stopImplicitCluster()
  stopCluster(cl)
}
```


Summary:


Our baseline model consists of four steps
1. Feature Extraction
2. Training the model
3. Model testing and comparison and enhance results.
4. Enhaced results 


Our baseline model

We ran feature extraction by sampling 1000 points from the LR image pixels and takin the 8 neighboring pixels for each point as features. 

Our Extraction time is 27.994s with an optimal depth of 11. Our cross-validation error is 0.0024515.
It is faster than the nerest-neighbor method which uses the value of nearby translated pixel values for the output pixel values. There was a resolution time od 109.8 s and a MSE of 0.004035 and PSNR of 25.3926.
It is also faster than bilinear interpolation which uses the weighted average of two translated pixel values for each output pixel value. The extraction time for bileanear is 121.2 s and the MSE is 0.003277 and the PSNR is 26.3723.

Additionally, it is also faster than bicubic interpolation which uses the weighted average of four translated pixel values for each output pixel value. The extraction time for bileanear is 183.6 s and the MSE is 0.004927 and the PSNR is 24.1553. 
  
  
  
Our improved models

**XGBooost**-

Advantages: 

-More regularized model formation to control over-fitting 

-Extremely faster  training process 



Controlled parameters: depth={4,5,6,7,8,9,10,11), eta={0.02,0.04,0.06,0.08,0.1}

Largest Depth among 12 classifiers = 10

Largest eta among 12 classifiers= 0.02

Training Time: 303s



Feature Extraction Time 105.349s

Superresolution time 5043s

The MSE of this model is 0.004146

The psnr is 25.0000



**ESPCN**

ESPCN uses  subpixel convolutional neural network layer for upscaling. This layer essentially uses regular convolutional layers followed by a specific type of image reshaping called a phase shift. In other words, instead of putting zeros in between pixels and having to do extra computation, they calculate convolutions in lower resolution and resize the resulting map into an upscaled image in the end. This way, no meaningless zeros are necessary. This makes it faster than SRCNN Network. SRCNN Network does more convolutions in high resolution, as it goes through CNN; this is more costly.

The feature extraction  time for this model was

The training time

Super resolution time

The MSE  is  0.002452
The psnr is 28.11563