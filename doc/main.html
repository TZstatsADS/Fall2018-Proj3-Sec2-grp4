<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8"/>

<title>Parallel Computing Setup</title>

<script type="text/javascript">
window.onload = function() {
  var imgs = document.getElementsByTagName('img'), i, img;
  for (i = 0; i < imgs.length; i++) {
    img = imgs[i];
    // center an image if it is the only element of its parent
    if (img.parentElement.childElementCount === 1)
      img.parentElement.style.textAlign = 'center';
  }
};
</script>





<style type="text/css">
body, td {
   font-family: sans-serif;
   background-color: white;
   font-size: 13px;
}

body {
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 20px;
}

tt, code, pre {
   font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace;
}

h1 {
   font-size:2.2em;
}

h2 {
   font-size:1.8em;
}

h3 {
   font-size:1.4em;
}

h4 {
   font-size:1.0em;
}

h5 {
   font-size:0.9em;
}

h6 {
   font-size:0.8em;
}

a:visited {
   color: rgb(50%, 0%, 50%);
}

pre, img {
  max-width: 100%;
}
pre {
  overflow-x: auto;
}
pre code {
   display: block; padding: 0.5em;
}

code {
  font-size: 92%;
  border: 1px solid #ccc;
}

code[class] {
  background-color: #F8F8F8;
}

table, td, th {
  border: none;
}

blockquote {
   color:#666666;
   margin:0;
   padding-left: 1em;
   border-left: 0.5em #EEE solid;
}

hr {
   height: 0px;
   border-bottom: none;
   border-top-width: thin;
   border-top-style: dotted;
   border-top-color: #999999;
}

@media print {
   * {
      background: transparent !important;
      color: black !important;
      filter:none !important;
      -ms-filter: none !important;
   }

   body {
      font-size:12pt;
      max-width:100%;
   }

   a, a:visited {
      text-decoration: underline;
   }

   hr {
      visibility: hidden;
      page-break-before: always;
   }

   pre, blockquote {
      padding-right: 1em;
      page-break-inside: avoid;
   }

   tr, img {
      page-break-inside: avoid;
   }

   img {
      max-width: 100% !important;
   }

   @page :left {
      margin: 15mm 20mm 15mm 10mm;
   }

   @page :right {
      margin: 15mm 10mm 15mm 20mm;
   }

   p, h2, h3 {
      orphans: 3; widows: 3;
   }

   h2, h3 {
      page-break-after: avoid;
   }
}
</style>



</head>

<body>
<p>In your final repo, there should be an R markdown file that organizes <strong>all computational steps</strong> for evaluating your proposed image classification framework. </p>

<p>This file is currently a template for running evaluation experiments of image analysis (or any predictive modeling). You should update it according to your codes but following precisely the same structure. </p>

<pre><code class="r">if(!require(&quot;EBImage&quot;)){
  source(&quot;https://bioconductor.org/biocLite.R&quot;)
  biocLite(&quot;EBImage&quot;)
  library(&quot;EBImage&quot;)
}
</code></pre>

<pre><code>## Loading required package: EBImage
</code></pre>

<pre><code class="r">if(!require(&quot;gbm&quot;)){
  install.packages(&quot;gbm&quot;)
  library(&quot;gbm&quot;)
}
</code></pre>

<pre><code>## Loading required package: gbm
</code></pre>

<pre><code>## Loaded gbm 2.1.4
</code></pre>

<pre><code class="r"># if(!require(&quot;png&quot;)){
#   install.packages(&quot;png&quot;)
#   library(&quot;png&quot;)
# }
</code></pre>

<h3>Parallel Computing Setup</h3>

<pre><code class="r">if(!require(&quot;doParallel&quot;)){
  install.packages(&quot;doParallel&quot;)
  library(&quot;doParallel&quot;)
}
</code></pre>

<pre><code>## Loading required package: doParallel
</code></pre>

<pre><code>## Loading required package: foreach
</code></pre>

<pre><code>## Loading required package: iterators
</code></pre>

<pre><code>## Loading required package: parallel
</code></pre>

<pre><code class="r"># Real physical cores in the computer
cores &lt;- detectCores()

if(cores&gt;1){

  if(.Platform$OS.type==&quot;windows&quot;){
    cl &lt;- makeCluster(cores)
    registerDoParallel(cl, cores=cores)
  }
  else{
    if(!require(&quot;doMC&quot;)){
      install.packages(&quot;doMC&quot;)
      library(&quot;doMC&quot;)
    }
    registerDoMC(cores)
  }

  run.parallel=TRUE

}else
  run.parallel=FALSE
</code></pre>

<pre><code>## Loading required package: doMC
</code></pre>

<h3>Step 0: specify directories.</h3>

<p>Set the working directory to the image folder. Specify the training and the testing set. For data without an independent test/validation set, you need to create your own testing data by random subsampling. In order to obain reproducible results, set.seed() whenever randomization is used. </p>

<pre><code class="r">set.seed(2018)
# setwd(&quot;./ads_fall2018_proj3&quot;) 
# here replace it with your own path or manually set it in RStudio to where this rmd file is located. 
# use relative path for reproducibility
</code></pre>

<p>Provide directories for training images. Low-resolution (LR) image set and High-resolution (HR) image set will be in different subfolders. </p>

<pre><code class="r">train_dir &lt;- &quot;../data/train_set/&quot; # This will be modified for different data sets.
train_LR_dir &lt;- paste(train_dir, &quot;LR/&quot;, sep=&quot;&quot;)
train_HR_dir &lt;- paste(train_dir, &quot;HR/&quot;, sep=&quot;&quot;)
train_label_path &lt;- paste(train_dir, &quot;label.csv&quot;, sep=&quot;&quot;) 
</code></pre>

<h3>Step 1: set up controls for evaluation experiments.</h3>

<p>In this chunk, we have a set of controls for the evaluation experiments. </p>

<ul>
<li>(T/F) cross-validation on the training set</li>
<li>(number) K, the number of CV folds</li>
<li>(T/F) process features for training set</li>
<li>(T/F) run evaluation on an independent test set</li>
<li>(T/F) process features for test set</li>
</ul>

<pre><code class="r">run.cv=TRUE # run cross-validation on the training set
K &lt;- 5  # number of CV folds
run.train=TRUE # New switch, whether run training.
run.feature.train=TRUE # process features for training set
run.test=TRUE # run evaluation on an independent test set
run.feature.test=TRUE # process features for test set
# run.parallel already set at ### Parallel Computing Setup
</code></pre>

<p>Using cross-validation or independent test set evaluation, we compare the performance of models with different specifications. In this example, we use GBM with different <code>depth</code>. In the following chunk, we list, in a vector, setups (in this case, <code>depth</code>) corresponding to models that we will compare. In your project, you might compare very different classifiers. You can assign them numerical IDs and labels specific to your project. </p>

<pre><code class="r">model_values &lt;- seq(1, 11, 2)
model_labels = paste(&quot;GBM with depth =&quot;, model_values)
</code></pre>

<h3>Step 2: import training images class labels.</h3>

<p>We provide extra information of image label: car (0), flower (1), market (2). These labels are not necessary for your model.</p>

<pre><code class="r">extra_label &lt;- read.csv(train_label_path, colClasses=c(&quot;NULL&quot;, NA, NA))
</code></pre>

<h3>Step 3: construct features and responses</h3>

<p><code>feature.R</code> should be the wrapper for all your feature engineering functions and options. The function <code>feature( )</code> should have options that correspond to different scenarios for your project and produces an R object that contains features and responses that are required by all the models you are going to evaluate later. </p>

<ul>
<li><code>feature.R</code>

<ul>
<li>Input: a path for low-resolution images.</li>
<li>Input: a path for high-resolution images.</li>
<li>Output: an RData file that contains extracted features and corresponding responses</li>
</ul></li>
</ul>

<p>I do recommend using non-parallel version here, as extract feature is a data-intensive --- not computation intensive step. Thus, parallel computing won&#39;t significantly save time --- even increase running time, for the parallel computing will raise additional coordination cost, depends on different hardwares: </p>

<table><thead>
<tr>
<th>Running Time</th>
<th align="center">HPC</th>
<th align="center">Pixelbook</th>
<th align="center">old labtop</th>
</tr>
</thead><tbody>
<tr>
<td>Parallel</td>
<td align="center">19</td>
<td align="center">55</td>
<td align="center">80</td>
</tr>
<tr>
<td>Non-Parallel</td>
<td align="center">51</td>
<td align="center">33</td>
<td align="center">90</td>
</tr>
</tbody></table>

<pre><code class="r">if(run.parallel)
  source(&quot;../lib/feature_parallel.R&quot;) else
    source(&quot;../lib/feature.R&quot;)
</code></pre>

<pre><code class="r">tm_feature_train &lt;- NA
if(run.feature.train){
  tm0=proc.time()
  dat_train &lt;- feature(train_LR_dir, train_HR_dir)
  tm_feature_train=proc.time()-tm0

  save(dat_train, file=&quot;../output/feature_train.RData&quot;)
}else{
  load(&quot;../output/feature_train.RData&quot;)
}
</code></pre>

<pre><code>## 
## Attaching package: &#39;abind&#39;
</code></pre>

<pre><code>## The following object is masked from &#39;package:EBImage&#39;:
## 
##     abind
</code></pre>

<pre><code class="r">feat_train &lt;- dat_train$feature
label_train &lt;- dat_train$label
rm(dat_train)
tmp=gc() # release memory
</code></pre>

<h3>Step 4: Train a classification model with training images</h3>

<p>Call the train model and test model from library. </p>

<p><code>train.R</code> and <code>test.R</code> should be wrappers for all your model training steps and your classification/prediction steps. </p>

<ul>
<li><code>train.R</code>

<ul>
<li>Input: a path that points to the training set features and responses.</li>
<li>Output: an RData file that contains trained classifiers in the forms of R objects: models/settings/links to external trained configurations.</li>
</ul></li>
<li><code>test.R</code>

<ul>
<li>Input: a path that points to the test set features.</li>
<li>Input: an R object that contains a trained classifier.</li>
<li>Output: an R object of response predictions on the test set. If there are multiple classifiers under evaluation, there should be multiple sets of label predictions. </li>
</ul></li>
</ul>

<pre><code class="r">if(run.parallel)
  source(&quot;../lib/train_parallel.R&quot;) else
    source(&quot;../lib/train.R&quot;)

### The test parallel will increase much more time, so we switch off (comment) it
# if(run.parallel)
#   source(&quot;../lib/test_parallel.R&quot;) else
    source(&quot;../lib/test.R&quot;)
</code></pre>

<h4>Model selection with cross-validation</h4>

<ul>
<li>Do model selection by choosing among different values of training model parameters, that is, the interaction depth for GBM in this example. </li>
</ul>

<pre><code class="r">source(&quot;../lib/cross_validation.R&quot;)

if(run.cv){
  err_cv &lt;- array(dim=c(length(model_values), 2))
  for(k in 1:length(model_values)){
    cat(&quot;k=&quot;, k, &quot;\n&quot;)
    err_cv[k,] &lt;- cv.function(feat_train, label_train, model_values[k], K)
    system(&quot;free -m&quot;)
  }
  save(err_cv, file=&quot;../output/err_cv.RData&quot;)
}else{
  load(&quot;../output/err_cv.RData&quot;)
}
</code></pre>

<pre><code>## k= 1 
## k= 2 
## k= 3 
## k= 4 
## k= 5 
## k= 6
</code></pre>

<pre><code class="r">tmp=gc() # release memory
</code></pre>

<p>Visualize cross-validation results. </p>

<pre><code class="r"># if(run.cv){
  # load(&quot;../output/err_cv.RData&quot;)
  plot(model_values, err_cv[,1], xlab=&quot;Interaction Depth&quot;, ylab=&quot;CV Error&quot;,
       main=&quot;Cross Validation Error&quot;, type=&quot;n&quot;, ylim=c(0, 0.005))
  points(model_values, err_cv[,1], col=&quot;blue&quot;, pch=16)
  lines(model_values, err_cv[,1], col=&quot;blue&quot;)
  arrows(model_values, err_cv[,1]-err_cv[,2], model_values, err_cv[,1]+err_cv[,2], 
        length=0.1, angle=90, code=3)
</code></pre>

<p><img src="figure/cv_vis-1.png" alt="plot of chunk cv_vis"></p>

<pre><code class="r"># }
</code></pre>

<ul>
<li>Choose the &quot;best&quot; parameter value</li>
</ul>

<pre><code class="r">model_best=model_values[1]
# if(run.cv){
  model_best &lt;- model_values[which.min(err_cv[,1])]
  cat(&quot;The min cv is&quot;,min(err_cv[,1]),&quot;for depth&quot;,model_best,&quot;\n&quot;)
</code></pre>

<pre><code>## The min cv is 0.002458547 for depth 11
</code></pre>

<pre><code class="r"># }

par_best &lt;- list(depth=model_best)
</code></pre>

<ul>
<li>Train the model with the entire training set using the selected model (model parameter) via cross-validation.</li>
</ul>

<pre><code class="r">if(run.train){
  tm0=proc.time()
  fit_train &lt;- train(feat_train, label_train, par_best)
  tm_train=proc.time()-tm0
  save(fit_train, file=&quot;../output/fit_train.RData&quot;)
}else{
  load(&quot;../output/fit_train.RData&quot;)
}
tmp=gc() # release memory
</code></pre>

<h3>Step 5: Super-resolution for test images</h3>

<p>Feed the final training model with the completely holdout testing data. </p>

<ul>
<li><code>superResolution.R</code>

<ul>
<li>Input: a path that points to the folder of low-resolution test images.</li>
<li>Input: a path that points to the folder (empty) of high-resolution test images.</li>
<li>Input: an R object that contains tuned predictors.</li>
<li>Output: construct high-resolution versions for each low-resolution test image.</li>
</ul></li>
</ul>

<pre><code class="r">if(run.parallel)
  source(&quot;../lib/superResolution_parallel.R&quot;) else
    source(&quot;../lib/superResolution.R&quot;)
test_dir &lt;- &quot;../data/test_set/&quot; # This will be modified for different data sets.
test_LR_dir &lt;- paste(test_dir, &quot;LR/&quot;, sep=&quot;&quot;)
test_HR_dir &lt;- paste(test_dir, &quot;HR/&quot;, sep=&quot;&quot;)

if(run.test){
  tm0=proc.time()
  superResolution(test_LR_dir, test_HR_dir, fit_train)
  tm_test=proc.time()-tm0
}
</code></pre>

<h3>Summarize Running Time</h3>

<p>Prediction performance matters, so does the running times for constructing features and for training the model, especially when the computation resource is limited. </p>

<pre><code class="r">cat(&quot;Time for constructing training features=&quot;, tm_feature_train[3], &quot;s \n&quot;)
</code></pre>

<pre><code>## Time for constructing training features= 28.419 s
</code></pre>

<pre><code class="r">#cat(&quot;Time for constructing testing features=&quot;, tm_feature_test[3], &quot;s \n&quot;)
cat(&quot;Time for training model=&quot;, tm_train[3], &quot;s \n&quot;)
</code></pre>

<pre><code>## Time for training model= 1500.212 s
</code></pre>

<pre><code class="r">cat(&quot;Time for super-resolution=&quot;, tm_test[3], &quot;s \n&quot;)
</code></pre>

<pre><code>## Time for super-resolution= 768.291 s
</code></pre>

<h3>Summarize Accuracy</h3>

<pre><code class="r">source(&quot;../lib/mse_psnr.R&quot;)
mp=msepsnr()
cat(&quot;MSE is&quot;, mp[1])
</code></pre>

<pre><code>## MSE is 0.002539523
</code></pre>

<pre><code class="r">cat(&quot;PSNR is&quot;, mp[2])
</code></pre>

<pre><code>## PSNR is 27.90005
</code></pre>

<h3>Stop Parallel Computing</h3>

<pre><code class="r">if(run.parallel &amp; .Platform$OS.type==&quot;windows&quot;){
  stopImplicitCluster()
  stopCluster(cl)
}
</code></pre>

</body>

</html>
